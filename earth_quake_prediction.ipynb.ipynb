{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4394246b-3b18-496e-a6ac-0c0fd8255595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Imports\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pickle\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6def4ad1-97b9-4f9e-a20d-cdd69a0e25b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Load and clean data\n",
    "df = pd.read_csv('Earthquake_Data.csv', delimiter=r'\\s+')\n",
    "\n",
    "# Rename columns for better readability\n",
    "new_column_names = [\"Date(YYYY/MM/DD)\", \"Time(UTC)\", \"Latitude(deg)\", \"Longitude(deg)\", \"Depth(km)\",\n",
    "                    \"Magnitude(ergs)\", \"Magnitude_type\", \"No_of_Stations\", \"Gap\", \"Close\", \"RMS\",\n",
    "                    \"SRC\", \"EventID\"]\n",
    "df.columns = new_column_names\n",
    "\n",
    "# Combine Date and Time columns into a datetime object and set as index\n",
    "ts = pd.to_datetime(df[\"Date(YYYY/MM/DD)\"] + \" \" + df[\"Time(UTC)\"])\n",
    "df = df.drop([\"Date(YYYY/MM/DD)\", \"Time(UTC)\"], axis=1)\n",
    "df.index = ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df70c86-6412-4491-88a5-8ce02bac3c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 18030 entries, 1966-07-01 09:41:21.820000 to 2007-12-28 23:20:28.120000\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Latitude(deg)    18030 non-null  float64\n",
      " 1   Longitude(deg)   18030 non-null  float64\n",
      " 2   Depth(km)        18030 non-null  float64\n",
      " 3   Magnitude(ergs)  18030 non-null  float64\n",
      " 4   Magnitude_type   18030 non-null  object \n",
      " 5   No_of_Stations   18030 non-null  int64  \n",
      " 6   Gap              18030 non-null  int64  \n",
      " 7   Close            18030 non-null  int64  \n",
      " 8   RMS              18030 non-null  float64\n",
      " 9   SRC              18030 non-null  object \n",
      " 10  EventID          18030 non-null  int64  \n",
      "dtypes: float64(5), int64(4), object(2)\n",
      "memory usage: 1.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# STEP 2.1: EDA (Exploratory Data Analysis)\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d592a8d-2056-43cc-95de-bb1900ee147f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics:\n",
      "       Latitude(deg)  Longitude(deg)     Depth(km)  Magnitude(ergs)  \\\n",
      "count   18030.000000    18030.000000  18030.000000     18030.000000   \n",
      "mean       37.639650     -120.921935      8.876301         3.427692   \n",
      "std         1.921843        2.341440      7.698564         0.437849   \n",
      "min        31.725700     -127.507000      0.000000         3.000000   \n",
      "25%        36.463700     -122.004325      4.860000         3.110000   \n",
      "50%        37.454500     -121.080700      7.070000         3.300000   \n",
      "75%        38.829925     -118.862000     10.590000         3.600000   \n",
      "max        45.562700     -112.107200    121.310000         7.390000   \n",
      "\n",
      "       No_of_Stations           Gap         Close           RMS       EventID  \n",
      "count    18030.000000  18030.000000  18030.000000  18030.000000  1.803000e+04  \n",
      "mean        31.943150    147.555851     28.965225      0.142098  6.515898e+06  \n",
      "std         24.535714     90.675337     42.751417      0.807726  1.233624e+07  \n",
      "min          4.000000     12.000000      0.000000      0.000000 -5.000034e+06  \n",
      "25%         14.000000     66.000000      4.000000      0.050000 -1.031021e+06  \n",
      "50%         26.000000    127.000000     10.000000      0.080000  5.540700e+04  \n",
      "75%         43.000000    230.000000     34.000000      0.120000  2.009116e+07  \n",
      "max        327.000000    354.000000    296.000000     45.400000  7.107032e+07  \n"
     ]
    }
   ],
   "source": [
    "print(\"Summary Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09de8cff-1b6f-40f8-adac-d9b21532b679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values:\n",
      "Latitude(deg)      0\n",
      "Longitude(deg)     0\n",
      "Depth(km)          0\n",
      "Magnitude(ergs)    0\n",
      "Magnitude_type     0\n",
      "No_of_Stations     0\n",
      "Gap                0\n",
      "Close              0\n",
      "RMS                0\n",
      "SRC                0\n",
      "EventID            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Null Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a56d2fef-47c0-4c78-ab77-735352c81132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magnitude Type Counts:\n",
      "Magnitude_type\n",
      "Md    12353\n",
      "ML     5499\n",
      "Mw       97\n",
      "Mx       81\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Source (SRC) Counts:\n",
      "SRC\n",
      "NCSN    18030\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in categorical columns\n",
    "if 'Magnitude_type' in df.columns:\n",
    "    print(\"Magnitude Type Counts:\")\n",
    "    print(df['Magnitude_type'].value_counts())\n",
    "\n",
    "if 'SRC' in df.columns:\n",
    "    print(\"\\nSource (SRC) Counts:\")\n",
    "    print(df['SRC'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08d150c2-c9c5-4317-bf71-0d652278efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Encode categorical columns\n",
    "df['Magnitude_type'] = LabelEncoder().fit_transform(df['Magnitude_type'])\n",
    "df['SRC'] = LabelEncoder().fit_transform(df['SRC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d2c9add-1783-457f-8403-e4cd5f24c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Feature Engineering\n",
    "\n",
    "# 4.1: Time-based features\n",
    "df['Hour'] = df.index.hour\n",
    "df['Day'] = df.index.day\n",
    "df['Month'] = df.index.month\n",
    "df['Year'] = df.index.year\n",
    "\n",
    "# 4.2: Magnitude Category\n",
    "df['Magnitude_category'] = pd.cut(df['Magnitude(ergs)'], \n",
    "                                  bins=[0, 5.0, 6.0, 7.0, 10], \n",
    "                                  labels=['Low', 'Moderate', 'Strong', 'Severe'])\n",
    "df['Magnitude_category'] = LabelEncoder().fit_transform(df['Magnitude_category'].astype(str))\n",
    "\n",
    "# 4.3: Latitude and Longitude Zones\n",
    "df['Lat_zone'] = pd.cut(df['Latitude(deg)'], bins=5, labels=False)\n",
    "df['Lon_zone'] = pd.cut(df['Longitude(deg)'], bins=5, labels=False)\n",
    "\n",
    "# 4.4: Depth Category\n",
    "df['Depth_category'] = pd.cut(df['Depth(km)'], bins=[0, 70, 300, 700], labels=['Shallow', 'Intermediate', 'Deep'])\n",
    "df['Depth_category'] = LabelEncoder().fit_transform(df['Depth_category'].astype(str))\n",
    "\n",
    "# 4.5: Interaction Features\n",
    "df['Station_Gap'] = df['No_of_Stations'] * df['Gap']\n",
    "df['Depth_Close'] = df['Depth(km)'] / (df['Close'] + 1e-6)  # Avoid division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0de81f1e-fcdb-4b63-acaa-b9f494788eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Feature Selection\n",
    "X = df.drop(columns=['Magnitude(ergs)', 'EventID'])  # Drop target and ID columns\n",
    "y = df['Magnitude(ergs)']  # Set target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b97806c1-fa7d-4cc5-b40c-12916e738b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f4a5e81-205e-4eaf-b621-8ebcc18df9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Evaluation Metrics:\n",
      "Mean Squared Error (MSE): 0.1413\n",
      "RÂ² Score: 0.2660\n",
      "Mean Absolute Error (MAE): 0.2883\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: Train Baseline Model (Linear Regression)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Train a simple linear regression model as the baseline model\n",
    "baseline_model = LinearRegression()\n",
    "\n",
    "# Fit the baseline model\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the baseline model\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for the baseline model\n",
    "mse_baseline = mean_squared_error(y_test, y_pred_baseline)\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
    "\n",
    "# Print metrics for the baseline model\n",
    "print(\"Baseline Model Evaluation Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_baseline:.4f}\")\n",
    "print(f\"RÂ² Score: {r2_baseline:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_baseline:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c8cf98c-6132-4c57-840f-4726adaca0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    }
   ],
   "source": [
    "# STEP 8: Hyperparameter Tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 15],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=0),\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='r2',\n",
    "                           cv=3,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_  # Best model from grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b66330be-1d42-48dd-b15f-3f364d118030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor Model Evaluation Metrics:\n",
      "Mean Squared Error (MSE): 0.1056\n",
      "RÂ² Score: 0.4511\n",
      "Mean Absolute Error (MAE): 0.2445\n"
     ]
    }
   ],
   "source": [
    "# STEP 9: Evaluate Model\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Random Forest Regressor Model Evaluation Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"RÂ² Score: {r2:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eef7d7f3-b8c9-4e44-a067-9c4027427eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â Model Saved Successfully\n"
     ]
    }
   ],
   "source": [
    "# STEP 10: Save Model\n",
    "with open('earthquake_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "    print(\"â Model Saved Successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14328c63-a7f9-4810-ac5b-26bf88975133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95421e5d-f1d3-4f54-803b-07f098eb4d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "with open('earthquake_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "211a01e1-184f-429c-b677-239d2b46981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify risk based on magnitude\n",
    "def classify_risk(magnitude):\n",
    "    if magnitude < 2.0:\n",
    "        return \"No Risk\"\n",
    "    elif 2.0 <= magnitude < 4.0:\n",
    "        return \"Very Low Risk\"\n",
    "    elif 4.0 <= magnitude < 5.0:\n",
    "        return \"Low Risk\"\n",
    "    elif 5.0 <= magnitude < 6.0:\n",
    "        return \"Moderate Risk\"\n",
    "    elif 6.0 <= magnitude < 7.0:\n",
    "        return \"High Risk\"\n",
    "    elif 7.0 <= magnitude < 8.0:\n",
    "        return \"Severe Risk\"\n",
    "    else:\n",
    "        return \"Extreme Risk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4f20f39-791a-478a-8859-053ffbb4c5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Predict Earthquake Magnitude using Loaded Model ---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many predictions do you want to make?  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter details for data point 1:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Latitude(deg):  23.45\n",
      "Longitude(deg):  78.23\n",
      "Depth(km):  10.0\n",
      "No_of_Stations:  15\n",
      "Gap:  45.0\n",
      "Close:  12.3\n",
      "RMS:  2.5\n",
      "Magnitude_type (as encoded number):  0\n",
      "SRC (as encoded number):  1\n",
      "Enter DateTime (YYYY/MM/DD HH:MM:SS):  2023/05/18 13:45:00\n"
     ]
    }
   ],
   "source": [
    "# STEP C: Take input and predict using loaded model\n",
    "print(\"\\n--- Predict Earthquake Magnitude using Loaded Model ---\")\n",
    "n = int(input(\"How many predictions do you want to make? \"))\n",
    "new_data = []\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for i in range(n):\n",
    "    print(f\"\\nEnter details for data point {i+1}:\")\n",
    "    latitude = float(input(\"Latitude(deg): \"))\n",
    "    longitude = float(input(\"Longitude(deg): \"))\n",
    "    depth = float(input(\"Depth(km): \"))\n",
    "    stations = int(input(\"No_of_Stations: \"))\n",
    "    gap = float(input(\"Gap: \"))\n",
    "    close = float(input(\"Close: \"))\n",
    "    rms = float(input(\"RMS: \"))\n",
    "    mag_type = int(input(\"Magnitude_type (as encoded number): \"))\n",
    "    src = int(input(\"SRC (as encoded number): \"))\n",
    "    \n",
    "    # Enter DateTime (this will be used for Hour, Day, Month, Year)\n",
    "    date_time_str = input(\"Enter DateTime (YYYY/MM/DD HH:MM:SS): \")\n",
    "    date_time_obj = datetime.strptime(date_time_str, '%Y/%m/%d %H:%M:%S')\n",
    "    \n",
    "    # Extract Hour, Day, Month, Year\n",
    "    hour = date_time_obj.hour\n",
    "    day = date_time_obj.day\n",
    "    month = date_time_obj.month\n",
    "    year = date_time_obj.year\n",
    "        # Feature engineering (replicating the transformations from the training data)\n",
    "    lat_zone = pd.cut([latitude], bins=5, labels=False)[0]\n",
    "    lon_zone = pd.cut([longitude], bins=5, labels=False)[0]\n",
    "    \n",
    "    # Magnitude category (you can map ranges directly as in the original code)\n",
    "    magnitude_category = pd.cut([rms], bins=[0, 5.0, 6.0, 7.0, 10], labels=['Low', 'Moderate', 'Strong', 'Severe'])\n",
    "    magnitude_category_encoded = label_encoder.fit_transform(magnitude_category.astype(str))[0]\n",
    "    \n",
    "    # Depth category (based on depth)\n",
    "    depth_category = pd.cut([depth], bins=[0, 70, 300, 700], labels=['Shallow', 'Intermediate', 'Deep'])\n",
    "    depth_category_encoded = label_encoder.fit_transform(depth_category.astype(str))[0]\n",
    "    \n",
    "    # Interaction features (same transformations from the training data)\n",
    "    station_gap = stations * gap\n",
    "    depth_close = depth / (close + 1e-6)  # Avoid division by zero\n",
    "    \n",
    "    # Creating a row of new input data for prediction\n",
    "    new_data.append([latitude, longitude, depth, stations, gap, close, rms, mag_type, src, \n",
    "                    lat_zone, lon_zone, magnitude_category_encoded, depth_category_encoded, \n",
    "                    station_gap, depth_close, hour, day, month, year])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "666bdb34-5d43-4e1e-b7f8-dbcf9a1c7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert new data into a DataFrame with the correct column order\n",
    "new_data_df = pd.DataFrame(new_data, columns=[\n",
    "    'Latitude(deg)', 'Longitude(deg)', 'Depth(km)', 'No_of_Stations', 'Gap', 'Close', 'RMS',\n",
    "    'Magnitude_type', 'SRC', 'Lat_zone', 'Lon_zone', 'Magnitude_category', 'Depth_category',\n",
    "    'Station_Gap', 'Depth_Close', 'Hour', 'Day', 'Month', 'Year'])\n",
    "\n",
    "# Ensure the columns in the new data match the order of training data\n",
    "training_columns = list(loaded_model.feature_names_in_)\n",
    "\n",
    "# Reindex the new DataFrame to match the training column order\n",
    "new_data_df = new_data_df.reindex(columns=training_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28214401-53c9-45d1-96a4-a810dbf20f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions using Loaded Model:\n",
      "Prediction 1: Magnitude(ergs) = 3.7343 \n",
      "The location is at : Very Low Risk\n"
     ]
    }
   ],
   "source": [
    "# Make prediction with the model\n",
    "predictions = loaded_model.predict(new_data_df)\n",
    "\n",
    "print(\"Predictions using Loaded Model:\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Prediction {i+1}: Magnitude(ergs) = {pred:.4f} \\nThe location is at : {classify_risk(pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7fac413-9352-497c-b25f-2208235dc922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Date(YYYY/MM/DD)         Time  Latitude  Longitude  Depth   Mag Magt  \\\n",
      "0           1966/07/01  09:41:21.82   35.9463  -120.4700  12.26  3.20   Mx   \n",
      "1           1966/07/02  12:08:34.25   35.7867  -120.3265   8.99  3.70   Mx   \n",
      "2           1966/07/02  12:16:14.95   35.7928  -120.3353   9.88  3.40   Mx   \n",
      "3           1966/07/02  12:25:06.12   35.7970  -120.3282   9.09  3.10   Mx   \n",
      "4           1966/07/05  18:54:54.36   35.9223  -120.4585   7.86  3.10   Mx   \n",
      "...                ...          ...       ...        ...    ...   ...  ...   \n",
      "18025       2007/12/19  12:14:09.62   34.1438  -116.9822   7.03  4.06   ML   \n",
      "18026       2007/12/21  12:14:56.45   37.3078  -121.6735   8.47  3.08   ML   \n",
      "18027       2007/12/23  21:43:43.54   37.2127  -117.8230  10.00  3.54   ML   \n",
      "18028       2007/12/28  01:59:42.40   36.5292  -121.1133   5.99  3.04   ML   \n",
      "18029       2007/12/28  23:20:28.12   38.7710  -122.7370   2.34  3.40   Mw   \n",
      "\n",
      "       Nst  Gap  Clo   RMS   SRC   EventID  \n",
      "0        7  171   20  0.02  NCSN  -4540462  \n",
      "1        8   86    3  0.04  NCSN  -4540520  \n",
      "2        8   89    2  0.03  NCSN  -4540521  \n",
      "3        8  101    3  0.08  NCSN  -4540522  \n",
      "4        9  161   14  0.04  NCSN  -4540594  \n",
      "...    ...  ...  ...   ...   ...       ...  \n",
      "18025   10   73   14  0.08  NCSN  40207706  \n",
      "18026  114   45    5  0.12  NCSN  51192926  \n",
      "18027   45  176   40  0.07  NCSN  51193070  \n",
      "18028   70   45    4  0.06  NCSN  51193343  \n",
      "18029   49   37    1  0.07  NCSN  51193419  \n",
      "\n",
      "[18030 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Earthquake_Data.csv', delimiter=r'\\s+', engine='python')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89e8d5f6-f993-4753-8ac5-2385a5856404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the trained model correctly\n",
    "model = joblib.load(\"earthquake_model.pkl\")  # Make sure the file is in the same folder\n",
    "print(\"model successfully loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "331cee5b-1117-44e2-9f7f-252686b03e97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
